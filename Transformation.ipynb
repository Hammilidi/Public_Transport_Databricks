{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Créez la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PublicTransport\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "storageaccount = \"fidelistorage\"\n",
    "container = \"public-transport\"\n",
    "key = \"Mea8hwXHm0gWaa3zhrgriV5EjqqECFidwHiUdaN3cqf0wfdP2/SmSEUaxCrNJuApXFIqQno2zJjG+AStx50KwQ==\"\n",
    "\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storageaccount}.dfs.core.windows.net\", \n",
    "    key\n",
    ")\n",
    "\n",
    "# Spécifiez le chemin du répertoire raw dans Azure Data Lake Storage Gen2\n",
    "raw_data_path = \"abfss://public-transport@fidelistorage.dfs.core.windows.net/raw/\"\n",
    "processed_path = \"abfss://public-transport@fidelistorage.dfs.core.windows.net/processed/\"\n",
    "\n",
    "# Utilisez dbutils.fs.ls pour lister les fichiers CSV bruts\n",
    "raw_files = dbutils.fs.ls(raw_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fonction pour effectuer les transformations sur un fichier\n",
    "def process_file(csv_file,count):\n",
    "    # Chargez le fichier CSV brut dans un DataFrame\n",
    "    raw_data = spark.read.format('csv').option('header', True).option('delimiter', ',').load(csv_file)\n",
    "\n",
    "    # Transformations de Date: Extraire l'année, le mois, le jour et le jour de la semaine de la date\n",
    "    transformed_data = raw_data.withColumn(\"Annee\", year(\"Date\")) \\\n",
    "                               .withColumn(\"Mois\", month(\"Date\")) \\\n",
    "                               .withColumn(\"Jour\", dayofmonth(\"Date\"))\n",
    "\n",
    "    # Catégorisation des retards\n",
    "    transformed_data = transformed_data.withColumn(\"CatégorieRetard\",\n",
    "        when(transformed_data[\"Delay\"] <= 0, \"Pas de Retard\")\n",
    "        .when((transformed_data[\"Delay\"] >= 1) & (transformed_data[\"Delay\"] <= 10), \"Retard Court\")\n",
    "        .when((transformed_data[\"Delay\"] >= 11) & (transformed_data[\"Delay\"] <= 20), \"Retard Moyen\")\n",
    "        .when(transformed_data[\"Delay\"] > 20, \"Long Retard\")\n",
    "        .otherwise(\"Autre\"))\n",
    "\n",
    "    # Détermination des heures de pointe\n",
    "    seuil_passagers_pointe = 75\n",
    "    transformed_data = transformed_data.withColumn(\"HeureDePointe\",\n",
    "        when(transformed_data[\"Passengers\"] > seuil_passagers_pointe, \"Oui\")\n",
    "        .otherwise(\"Non\"))\n",
    "    \n",
    "    # Analyse des itinéraires\n",
    "    analytical_data = transformed_data.groupBy(\"Route\").agg(\n",
    "        F.mean(\"Delay\").alias(\"RetardMoyen\"),\n",
    "        F.mean(\"Passengers\").alias(\"NombreMoyenPassagers\"),\n",
    "        F.count(\"*\").alias(\"NombreTotalVoyages\"))\n",
    "\n",
    "    csv_file_path = processed_path + \"data/transformed_data\"+str(count)\n",
    "    transformed_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "    return analytical_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "raw_csv_files = [f.path for f in raw_files if f.name.endswith(\".csv\")]\n",
    "\n",
    "# Définissez le nombre de fichiers à traiter à chaque itération\n",
    "batch_size = 2\n",
    "count = 1\n",
    "while raw_csv_files:\n",
    "    # Prenez les prochains fichiers à traiter dans la liste\n",
    "    batch_files = raw_csv_files[:batch_size]\n",
    "    \n",
    "    # Mettez à jour la liste des fichiers restants\n",
    "    raw_csv_files = raw_csv_files[batch_size:]\n",
    "    \n",
    "    # Boucle sur les fichiers pour effectuer les transformations sur chaque fichier\n",
    "    for csv_file in batch_files:\n",
    "        \n",
    "        # Chargez le fichier CSV brut dans un DataFrame\n",
    "        raw_data = spark.read.format('csv').option('header', True).option('delimiter', ',').load(csv_file)\n",
    "        \n",
    "        analytical_data = process_file(csv_file,count)\n",
    "        count = count + 1\n",
    "        print(analytical_data)\n",
    "        # Extrayez le nom du fichier (data2.csv) à partir du chemin complet\n",
    "        filename = os.path.basename(csv_file)\n",
    "        # print(filename)\n",
    "        # Enregistrez le DataFrame transformé dans un répertoire de données traitées avec le nom de fichier\n",
    "        analytical_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(processed_path + \"analyse/\" + filename)\n",
    "\n",
    "\n",
    "    # Attendez 15 secondes avant de traiter le prochain lot de fichiers\n",
    "    print(\"Attendez 15 secondes avant de traiter le prochain lot de fichiers !\")\n",
    "    time.sleep(15)\n",
    "\n",
    "# Fermez la session Spark\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
