{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df77cf7-29b9-4af4-ba15-cb5a303443ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créez la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PublicTransport\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "storageaccount = \"fidelistorage\"\n",
    "container = \"public-transport\"\n",
    "key = \"Mea8hwXHm0gWaa3zhrgriV5EjqqECFidwHiUdaN3cqf0wfdP2/SmSEUaxCrNJuApXFIqQno2zJjG+AStx50KwQ==\"\n",
    "\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storageaccount}.dfs.core.windows.net\", \n",
    "    key\n",
    ")\n",
    "\n",
    "# Spécifiez le chemin du répertoire raw dans Azure Data Lake Storage Gen2\n",
    "raw_data_path = \"abfss://public-transport@fidelistorage.dfs.core.windows.net/raw/\"\n",
    "processed_path = \"abfss://public-transport@fidelistorage.dfs.core.windows.net/processed/\"\n",
    "archives_path = \"abfss://public-transport@fidelistorage.dfs.core.windows.net/archives/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archivage des données brutes (raw data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bf1201-a9b9-497a-b140-64d2fe4ff170",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Récupérez la liste des fichiers dans le dossier raw/\n",
    "raw_files = dbutils.fs.ls(raw_data_path)\n",
    "\n",
    "# Obtenez la date actuelle pour créer un dossier d'archivage avec cette date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "archive_folder = os.path.join(archives_path, \"raw\", current_date)\n",
    "\n",
    "# Créez le dossier d'archivage pour les données brutes\n",
    "dbutils.fs.mkdirs(archive_folder)\n",
    "\n",
    "# Déplacez chaque fichier du dossier raw/ vers le dossier d'archivage pour les données brutes\n",
    "for file in raw_files:\n",
    "    file_name = os.path.basename(file.path)\n",
    "    archive_file_path = os.path.join(archive_folder, file_name)\n",
    "    dbutils.fs.mv(file.path, archive_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archivage des données traitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5ec0d8-ba3c-422b-8f28-1ea34b4ba69c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Récupérez la liste des fichiers dans le dossier processed/\n",
    "processed_files = dbutils.fs.ls(processed_path)\n",
    "\n",
    "# Obtenez la date actuelle pour créer un dossier d'archivage avec cette date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "archive_folder = os.path.join(archives_path, \"processed\", current_date)\n",
    "\n",
    "# Créez le dossier d'archivage pour les données transformées\n",
    "dbutils.fs.mkdirs(archive_folder)\n",
    "\n",
    "# Déplacez chaque fichier du dossier processed/ vers le dossier d'archivage pour les données transformées\n",
    "for file in processed_files:\n",
    "    file_name = os.path.basename(file.path)\n",
    "    archive_file_path = os.path.join(archive_folder, file_name)\n",
    "    dbutils.fs.mv(file.path, archive_file_path, recurse=True)\n",
    "\n",
    "# Vous pouvez également supprimer les fichiers du dossier processed/ si nécessaire\n",
    "# for file in processed_files:\n",
    "#     dbutils.fs.rm(file.path)\n",
    "\n",
    "# Fermez la session Spark\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "script conservation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
